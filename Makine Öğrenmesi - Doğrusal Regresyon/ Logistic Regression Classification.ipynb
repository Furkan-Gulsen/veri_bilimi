{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification\n",
    "- Binary Classification =>  0 veya 1\n",
    "- En küçük Neural Network.\n",
    "- Bu datayı şimdi dersi analatırken  anlatıcam. Classiifaction yaparken birden fazla data tipi var. Bunlar mesela image. İleride kanser datasını ayırt etmeyi öğreticez. \n",
    "- Datamız'da 0 ve 1 ile gösterilen resimler var. Bu şimdi 1. Bunun labeli 1. Benim datamda 1 olarak geçiyor. Daha sonra bu resimler px'lerden oluşuyor. Bu px'ler 0-1 arasında yer alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./img/linear_reg_classification.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"./img/linear_reg_classification.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Computation Graph\n",
    "- Matematiksel ifadeleri görselleştirmek için kullanırız.\n",
    "- a^2 + b^2 = Square Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./img/linear_reg_classification2.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"./img/linear_reg_classification2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Amaç:** Modeli test veya train etmek.\n",
    "- Benim 4096px boyutunda bir tane resmim var.\n",
    "- Train etmek ne demek => Bu resmi kendi modeline uygulamak\n",
    "- Benim px1,px2... yani bunlar 4096 feature. Bunların hepsini w1,w2... ile çarpıyoruz. Daha sonra SUM işlemi yapıyoruz.\n",
    "- b =>  bias\n",
    "- z = b + px1.w1 + px2.w2 ... px4096.w4096\n",
    "- Parametreler weight ve bias.\n",
    "- w'ler katsayıları temsil ediyor.\n",
    "- Sigmoid Function => Bizim sayımızı 0-1 arasında bir değere eşitler.\n",
    "- 0.5 üstü 1 olsun, 0.5 altı 0 olsun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Forward Propagation\n",
    "- z = (w.T)x + b <br> T = Transpose\n",
    "- Eğer sen 1 resmini yolladıysan(predict) senin loss değerin 0'dır. Eğer 0 yolladıysa ve bu da doğru çıkıtysa loss yine 0'dır. Yani doğrular 0 ile simgelenir.\n",
    "- -(1-y)log(1-y)-y.log(y)\n",
    "\n",
    "## Backward Propagation\n",
    "- Benim modelim weigth ve bias'ı öğrenmek zorunda. Bu parametreler için cost'u azaltıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./img/linear_reg_classification3.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"./img/linear_reg_classification3.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Azaltma işlemini w'leri azaltarak değerlerini yaparız.\n",
    "- w := w - step <br> step = slope1\n",
    "- slope => Eğim demektir.\n",
    "- Minumum noktaya ulaşana kadar w değerlerini azaltıyorum.\n",
    "- First Step  => w=5, slope1=3   |  w - slope1 = 5 - 3   = 2 , cost = 0.4 (not min) <br>\n",
    "  Second Step => w=2, slope2=0.7 |  w - slope2 = 2 - 0.7 = 1.3,  cost = 0.3 (not min) <br>\n",
    "  Third Step  => w=1.3, slope3=0.01 | w - slope3 = 1.3 - 0.01 = 1.29\n",
    "- Slope3=0.01 bakarsak burada neredeyse değişim yoktur. Bu durumda min noktaya ulaşılmıştır.\n",
    "- Backward propagation yaparken önemli olan parametreleri güncellemektir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"LogisticRegressionDataset.csv\",sep=\",\");\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> M = Kötü huylu tümör <br>\n",
    "> B = İyi huylu tümör"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      "id                         569 non-null int64\n",
      "diagnosis                  569 non-null object\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "Unnamed: 32                0 non-null float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data'nın içinde ki bazı veriere ihtiyacım yok. Boş olanları çıkartıyoruz ilk olarak. <br>\n",
    "> axis = 1 => column drop etmek <br>\n",
    "> axis = 0 => row drop etmek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['Unnamed: 32','id'],axis=1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> İyi ve kötü huylu tümörleri sayısal olarak ifade etmemiz data'yı daha iyi kontrol etmemizi sağlar. Aynı zamanda diagnasis satırı bir objedir. Ve objeleri sınıflandırmada kullanamıyoruz. Bu yüzden bunları sayısal ifadeye çeviriyorum.\n",
    "<br>\n",
    "> 1 => Kötü huylu <br> \n",
    "> 0 => İyi huylu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0            1        17.99         10.38          122.80     1001.0   \n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568          0         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.diagnosis.values\n",
    "x_data = data.drop([\"diagnosis\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalization: Tüm dataları 0-1 arasında ki değerlere atmaktır.\n",
    "x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hocanın anlattığı mantıkla aynı olsun diye yapılı. Bir mantığı yok.\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T.reshape(-1,1)\n",
    "y_test = y_test.T.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (30, 455)\n",
      "x_test (30, 114)\n",
      "y_train (455, 1)\n",
      "y_test (114, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train\",x_train.shape)\n",
    "print(\"x_test\",x_test.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter initiazlize and sigmoid function: featurları w ile çarpıp bias ile toplama\n",
    "# bu data'ya göre deimension = 30\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01) # np.full((row,column),value)\n",
    "    b = 0.0 # int değilde float olsun diye yazdı. Çok önemli değil.\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w,b = initialize_weights_and_bias(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "# z = 0-1 arasında bir değer döndürür\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z)) # exp = üstü , bu y_head formulden geliyor\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 değerini aldığında bize 0.5 dönmesi gerekiyor. Kontrol edelim.\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975273768433653"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 değerini aldığında 1'e yakın bir değer döndürmesi gerekiyor. Kontrol edelim.\n",
    "sigmoid(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w,b,x_train,y_train): # y_train=en sonda karşılaştrma için kullanılır\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b # (1,30)*(30,455)\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) # formül\n",
    "    cost = (np.sum(loss))/x_train.shape[1] # x_train.shape[1] = 455\n",
    "    \n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # cost sonucunun türevi,formül\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1] # bias sonucunun türevi\n",
    "    gradients = {\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n",
    "    \n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating(learning) parameters\n",
    "# learning_rate => öğrenme katsayısı\n",
    "# number_of_iterarion => kaç kez ileri geri yapacağım\n",
    "def update(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    \n",
    "    # updating(learning) parameters is number_of_iteration cost and gradients\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * gradients['derivative_weight']\n",
    "        b = b - learning_rate * gradients['derivative_bian']\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print(\"Cost after iteration %i: %f\" %(i,cost))\n",
    "            \n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\":w,\"bias\":b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation=\"vertical\")\n",
    "    plt.xlabel(\"Number of Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predict(w,b,x_test):\n",
    "    # x_test: kanser olup olmadığı belli olmayan 0-1 değerleri hariç bütün özellikler\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1])) # 0-1 değerlerini içinde tutar\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0)\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i] <= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 319.889363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mfurk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\Users\\mfurk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 10: nan\n",
      "Cost after iteration 20: 2432.105447\n",
      "Cost after iteration 30: nan\n",
      "Cost after iteration 40: nan\n",
      "Cost after iteration 50: nan\n",
      "Cost after iteration 60: nan\n",
      "Cost after iteration 70: nan\n",
      "Cost after iteration 80: nan\n",
      "Cost after iteration 90: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 62.280701754385966 %\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n",
    "    # initialize\n",
    "    dimension = x_train.shape[0] # that is 4096\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters['weight'],parameters['bias'],x_test)\n",
    "    \n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test))*100))\n",
    "    \n",
    "logistic_regression(x_train,y_train,x_test,y_test,learning_rate=1,num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [30, 455]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-3d01d7c6a23d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test accuracy: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1532\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [30, 455]"
     ]
    }
   ],
   "source": [
    "# Yukardakilerin kısa yolu, sklearn kullanımı\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(x_train.T,y_train.T)\n",
    "print(\"Test accuracy: {}\".format(lr.score(x_test.T,y_test.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      "diagnosis                  569 non-null int64\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n",
      "None\n",
      "x_train:  (30, 455)\n",
      "x_test:  (30, 114)\n",
      "y_train:  (455,)\n",
      "y_test:  (114,)\n",
      "Cost after iteration 0: 0.692977\n",
      "Cost after iteration 10: 0.499667\n",
      "Cost after iteration 20: 0.406616\n",
      "Cost after iteration 30: 0.351936\n",
      "Cost after iteration 40: 0.315762\n",
      "Cost after iteration 50: 0.289862\n",
      "Cost after iteration 60: 0.270257\n",
      "Cost after iteration 70: 0.254795\n",
      "Cost after iteration 80: 0.242214\n",
      "Cost after iteration 90: 0.231722\n",
      "Cost after iteration 100: 0.222796\n",
      "Cost after iteration 110: 0.215080\n",
      "Cost after iteration 120: 0.208317\n",
      "Cost after iteration 130: 0.202324\n",
      "Cost after iteration 140: 0.196961\n",
      "Cost after iteration 150: 0.192121\n",
      "Cost after iteration 160: 0.187722\n",
      "Cost after iteration 170: 0.183698\n",
      "Cost after iteration 180: 0.179997\n",
      "Cost after iteration 190: 0.176577\n",
      "Cost after iteration 200: 0.173402\n",
      "Cost after iteration 210: 0.170443\n",
      "Cost after iteration 220: 0.167676\n",
      "Cost after iteration 230: 0.165080\n",
      "Cost after iteration 240: 0.162638\n",
      "Cost after iteration 250: 0.160334\n",
      "Cost after iteration 260: 0.158155\n",
      "Cost after iteration 270: 0.156091\n",
      "Cost after iteration 280: 0.154131\n",
      "Cost after iteration 290: 0.152266\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 96.49122807017544 %\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jul 12 21:06:37 2018\n",
    "\n",
    "@author: user\n",
    "\"\"\"\n",
    "\n",
    "# %% libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %% read csv\n",
    "data = pd.read_csv(\"LogisticRegressionDataset.csv\")\n",
    "data.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace = True)\n",
    "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n",
    "print(data.info())\n",
    "\n",
    "y = data.diagnosis.values\n",
    "x_data = data.drop([\"diagnosis\"],axis=1)\n",
    "\n",
    "# %% normalization\n",
    "x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\n",
    "\n",
    "# (x - min(x))/(max(x)-min(x))\n",
    "\n",
    "# %% train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "print(\"x_train: \",x_train.shape)\n",
    "print(\"x_test: \",x_test.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print(\"y_test: \",y_test.shape)\n",
    "\n",
    "# %% parameter initialize and sigmoid function\n",
    "# dimension = 30\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    \n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w,b\n",
    "\n",
    "\n",
    "# w,b = initialize_weights_and_bias(30)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    y_head = 1/(1+ np.exp(-z))\n",
    "    return y_head\n",
    "# print(sigmoid(0))\n",
    "\n",
    "# %%\n",
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    \n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
    "    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
    "    \n",
    "    return cost,gradients\n",
    "\n",
    "#%% Updating(learning) parameters\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    \n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list\n",
    "\n",
    "#%%  # prediction\n",
    "def predict(w,b,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "# %% logistic_regression\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[0]  # that is 30\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "\n",
    "    # Print test Errors\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(x_train.T,y_train.T)\n",
    "print(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
